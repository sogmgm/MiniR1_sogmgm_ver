[project]
name = "minir1"
version = "0.1.0"
description = "Mini-R1: Reproduce DeepSeek R1 aha moment with GRPO on RunPod"
readme = "README.md"
requires-python = ">=3.10"
license = { text = "MIT" }

dependencies = [
    # Core ML libraries
    "transformers>=4.48.0",
    "datasets>=3.1.0",
    "accelerate>=1.3.0",
    "trl>=0.14.0",
    
    # Model optimization
    "peft>=0.14.0",
    "bitsandbytes>=0.45.0",
    
    # Utilities
    "huggingface-hub>=0.26.0",
    "hf-transfer>=0.1.8",
    "safetensors>=0.4.0",
    
    # Data processing
    "numpy>=1.24.0",
    "pyyaml>=6.0",
    "requests>=2.32.2",
    
    # Logging & monitoring
    "tqdm>=4.66.0",
    "rich>=13.0.0",
    "tensorboard>=2.14.0",
]

[project.optional-dependencies]
dev = [
    "jupyter>=1.0.0",
    "ipywidgets>=8.0.0",
    "matplotlib>=3.7.0",
]

flash-attn = [
    "flash-attn>=2.7.0",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

# Hatch build configuration
[tool.hatch.build.targets.wheel]
packages = ["scripts"]
only-include = ["scripts", "configs"]

[tool.hatch.build.targets.sdist]
include = [
    "scripts/",
    "configs/",
    "README.md",
    "RUNPOD_SETUP.md",
    "TENSORBOARD_GUIDE.md",
    "PROGRESS.md",
    "pyproject.toml",
]

# ============================================
# UV ì‚¬ìš© ê°€ì´ë“œ (RunPod)
# ============================================
# 
# 1. ì˜ì¡´ì„± ì„¤ì¹˜ (í”„ë¡œì íŠ¸ ë¹Œë“œ ì œì™¸):
#    uv sync --no-install-project
#
# 2. PyTorch í™•ì¸ (RunPod pytorch í…œí”Œë¦¿ ì‚¬ìš© ì‹œ ì´ë¯¸ ì„¤ì¹˜ë¨):
#    uv run python -c "import torch; print(torch.__version__); print(torch.cuda.is_available())"
#    â†’ "2.x.x+cu12x"ì™€ "True" ì¶œë ¥ ì‹œ 3ë²ˆ ê±´ë„ˆë›°ê¸°
#    â†’ "False" ë˜ëŠ” ì—ëŸ¬ ì‹œ:
#    uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
#
# 3. Flash Attention (ì„ íƒ, í•™ìŠµ ì†ë„ 20% í–¥ìƒ):
#    âš ï¸ ì„¤ì¹˜ ì‹œê°„: 5-15ë¶„ (GPUì— ë”°ë¼ ë‹¤ë¦„)
#    âš ï¸ ì„¤ì¹˜ ì‹¤íŒ¨ ì‹œ eager attentionìœ¼ë¡œ ëŒ€ì²´ ê°€ëŠ¥ (ì„±ëŠ¥ ì°¨ì´ ë¯¸ë¯¸)
#    
#    # ì„¤ì¹˜ ì‹œë„:
#    uv pip install flash-attn --no-build-isolation
#    
#    # ì§„í–‰ ìƒí™© í™•ì¸ (ë‹¤ë¥¸ í„°ë¯¸ë„):
#    top -p $(pgrep -f "pip install flash")
#    # %CPUê°€ 0%ì´ê³  5ë¶„ ì´ìƒ ì§€ì† ì‹œ ì¤‘ë‹¨:
#    kill -9 $(pgrep -f "pip install flash")
#    
#    # Eager attentionìœ¼ë¡œ ì „í™˜:
#    sed -i 's/flash_attention_2/eager/' configs/training_config.yaml
#
# 4. ì‹¤í–‰:
#    uv run python scripts/train_grpo.py --config configs/training_config.yaml
#
# ğŸ’¡ Tip: Flash Attention ì—†ì´ë„ í•™ìŠµ ê°€ëŠ¥í•©ë‹ˆë‹¤.
#         Eager attention ì‚¬ìš© ì‹œ ì†ë„ëŠ” 20% ëŠë¦¬ì§€ë§Œ ì•ˆì •ì ì…ë‹ˆë‹¤.
#
# ìƒì„¸ ê°€ì´ë“œ: RUNPOD_SETUP.md

