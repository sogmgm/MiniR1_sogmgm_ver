#!/usr/bin/env python3
"""
TensorBoard ë¡œê·¸ íŒŒì¼ ì½ê¸° ë° ë‚´ë³´ë‚´ê¸° ìŠ¤í¬ë¦½íŠ¸
í•™ìŠµ ì¤‘ ê¸°ë¡ëœ ë©”íŠ¸ë¦­ë“¤ì„ ì¶”ì¶œí•˜ê³  LLM ë¶„ì„ìš©ìœ¼ë¡œ ë‚´ë³´ëƒ…ë‹ˆë‹¤.
"""

import os
import json
from pathlib import Path
from typing import Dict, List, Tuple
import glob
from datetime import datetime

try:
    from tensorboard.backend.event_processing import event_accumulator
    from collections import defaultdict
    import numpy as np
except ImportError:
    print("âŒ tensorboard íŒ¨í‚¤ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
    print("ì„¤ì¹˜: pip install tensorboard")
    exit(1)


def read_tensorboard_events(log_dir: str) -> Dict[str, List[Tuple[int, float]]]:
    """
    TensorBoard ë¡œê·¸ ë””ë ‰í† ë¦¬ì—ì„œ ëª¨ë“  ì´ë²¤íŠ¸ë¥¼ ì½ì–´ì˜µë‹ˆë‹¤.
    
    Args:
        log_dir: TensorBoard ë¡œê·¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        
    Returns:
        ë©”íŠ¸ë¦­ ì´ë¦„ì„ í‚¤ë¡œ, (step, value) íŠœí”Œ ë¦¬ìŠ¤íŠ¸ë¥¼ ê°’ìœ¼ë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬
    """
    # ëª¨ë“  ì´ë²¤íŠ¸ íŒŒì¼ ì°¾ê¸°
    event_files = glob.glob(os.path.join(log_dir, "events.out.tfevents.*"))
    
    if not event_files:
        print(f"âš ï¸  {log_dir}ì—ì„œ ì´ë²¤íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return {}
    
    print(f"ğŸ“ ë°œê²¬ëœ ì´ë²¤íŠ¸ íŒŒì¼: {len(event_files)}ê°œ")
    
    # ëª¨ë“  ë©”íŠ¸ë¦­ ë°ì´í„°ë¥¼ ìˆ˜ì§‘
    all_metrics = defaultdict(list)
    
    for event_file in sorted(event_files):
        print(f"   ì½ëŠ” ì¤‘: {os.path.basename(event_file)}")
        
        try:
            # EventAccumulator ì´ˆê¸°í™”
            ea = event_accumulator.EventAccumulator(event_file)
            ea.Reload()
            
            # ì‚¬ìš© ê°€ëŠ¥í•œ íƒœê·¸(ë©”íŠ¸ë¦­) í™•ì¸
            tags = ea.Tags()
            
            # ìŠ¤ì¹¼ë¼ ê°’ ì½ê¸°
            for tag in tags['scalars']:
                events = ea.Scalars(tag)
                for event in events:
                    all_metrics[tag].append((event.step, event.value))
                    
        except Exception as e:
            print(f"   âš ï¸  íŒŒì¼ ì½ê¸° ì‹¤íŒ¨: {e}")
            continue
    
    # ê° ë©”íŠ¸ë¦­ì„ step ìˆœì„œë¡œ ì •ë ¬
    for tag in all_metrics:
        all_metrics[tag] = sorted(all_metrics[tag], key=lambda x: x[0])
    
    return dict(all_metrics)


def export_to_json(metrics: Dict[str, List[Tuple[int, float]]], output_path: str):
    """ë©”íŠ¸ë¦­ì„ JSON íŒŒì¼ë¡œ ë‚´ë³´ëƒ…ë‹ˆë‹¤."""
    
    data = {
        "export_time": datetime.now().isoformat(),
        "metrics": {}
    }
    
    for tag, values in metrics.items():
        if not values:
            continue
        
        steps, vals = zip(*values)
        
        data["metrics"][tag] = {
            "data_points": [{"step": s, "value": v} for s, v in values],
            "summary": {
                "count": len(values),
                "min_step": min(steps),
                "max_step": max(steps),
                "min_value": float(min(vals)),
                "max_value": float(max(vals)),
                "mean_value": float(np.mean(vals)),
                "final_value": float(vals[-1]),
                "final_step": int(steps[-1])
            }
        }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… JSON íŒŒì¼ ì €ì¥: {output_path}")


def export_to_markdown(metrics: Dict[str, List[Tuple[int, float]]], output_path: str, run_name: str = "Training Run"):
    """ë©”íŠ¸ë¦­ì„ Markdown íŒŒì¼ë¡œ ë‚´ë³´ëƒ…ë‹ˆë‹¤ (LLM ë¶„ì„ìš©)."""
    
    lines = []
    lines.append(f"# TensorBoard ë©”íŠ¸ë¦­ ë¶„ì„ ë¦¬í¬íŠ¸")
    lines.append(f"\n**Run Name:** {run_name}")
    lines.append(f"**Export Time:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append(f"**Total Metrics:** {len(metrics)}")
    lines.append("\n---\n")
    
    for tag, values in sorted(metrics.items()):
        if not values:
            continue
        
        steps, vals = zip(*values)
        
        lines.append(f"## ğŸ“Š {tag}\n")
        lines.append(f"- **ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜:** {len(values)}")
        lines.append(f"- **Step ë²”ìœ„:** {min(steps)} ~ {max(steps)}")
        lines.append(f"- **ìµœì†Œê°’:** {min(vals):.6f} (step {steps[vals.index(min(vals))]})")
        lines.append(f"- **ìµœëŒ€ê°’:** {max(vals):.6f} (step {steps[vals.index(max(vals))]})")
        lines.append(f"- **í‰ê· ê°’:** {np.mean(vals):.6f}")
        lines.append(f"- **ìµœì¢…ê°’:** {vals[-1]:.6f} (step {steps[-1]})")
        
        # ì „ì²´ ë°ì´í„° í…Œì´ë¸”
        lines.append(f"\n### ì „ì²´ ë°ì´í„° (Stepë³„)\n")
        lines.append("| Step | Value |")
        lines.append("|------|-------|")
        for step, val in values:
            lines.append(f"| {step} | {val:.6f} |")
        
        lines.append("\n---\n")
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(lines))
    
    print(f"âœ… Markdown íŒŒì¼ ì €ì¥: {output_path}")


def export_to_csv(metrics: Dict[str, List[Tuple[int, float]]], output_path: str):
    """ë©”íŠ¸ë¦­ì„ CSV íŒŒì¼ë¡œ ë‚´ë³´ëƒ…ë‹ˆë‹¤."""
    
    lines = []
    
    # í—¤ë”
    metric_names = sorted(metrics.keys())
    lines.append("step," + ",".join(metric_names))
    
    # ëª¨ë“  step ìˆ˜ì§‘
    all_steps = set()
    for values in metrics.values():
        all_steps.update(s for s, _ in values)
    
    # stepë³„ ë°ì´í„° ë§¤í•‘
    step_data = {step: {} for step in sorted(all_steps)}
    for tag, values in metrics.items():
        for step, val in values:
            step_data[step][tag] = val
    
    # CSV ì‘ì„±
    for step in sorted(step_data.keys()):
        row = [str(step)]
        for metric in metric_names:
            val = step_data[step].get(metric, "")
            row.append(f"{val:.6f}" if val != "" else "")
        lines.append(",".join(row))
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(lines))
    
    print(f"âœ… CSV íŒŒì¼ ì €ì¥: {output_path}")


def print_metric_summary(metrics: Dict[str, List[Tuple[int, float]]]):
    """ë©”íŠ¸ë¦­ ìš”ì•½ ì •ë³´ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
    
    if not metrics:
        print("âŒ ë©”íŠ¸ë¦­ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print("\n" + "="*80)
    print("ğŸ“Š TensorBoard ë©”íŠ¸ë¦­ ìš”ì•½")
    print("="*80)
    
    for tag, values in sorted(metrics.items()):
        if not values:
            continue
            
        steps, vals = zip(*values)
        
        print(f"\nğŸ“ˆ {tag}")
        print(f"   ì´ ë°ì´í„° í¬ì¸íŠ¸: {len(values)}ê°œ")
        print(f"   Step ë²”ìœ„: {min(steps)} ~ {max(steps)}")
        print(f"   ìµœì†Œê°’: {min(vals):.6f} (step {steps[vals.index(min(vals))]})") 
        print(f"   ìµœëŒ€ê°’: {max(vals):.6f} (step {steps[vals.index(max(vals))]})")
        print(f"   í‰ê· ê°’: {np.mean(vals):.6f}")
        print(f"   ìµœì¢…ê°’: {vals[-1]:.6f} (step {steps[-1]})")
        
        # ìµœê·¼ 10ê°œ ë°ì´í„° í¬ì¸íŠ¸ í‘œì‹œ
        print(f"   ìµœê·¼ 10ê°œ ë°ì´í„°:")
        for step, val in values[-10:]:
            print(f"      Step {step:4d}: {val:.6f}")


def compare_runs(log_dirs: List[str]):
    """ì—¬ëŸ¬ ì‹¤í–‰ì˜ ê²°ê³¼ë¥¼ ë¹„êµí•©ë‹ˆë‹¤. (ì½˜ì†” ì¶œë ¥ìš©)"""
    
    print("\n" + "="*80)
    print("ğŸ”„ ì—¬ëŸ¬ ì‹¤í–‰ ë¹„êµ")
    print("="*80)
    
    all_runs = {}
    
    for log_dir in log_dirs:
        run_name = os.path.basename(log_dir)
        print(f"\nğŸ“‚ {run_name} ì½ëŠ” ì¤‘...")
        metrics = read_tensorboard_events(log_dir)
        all_runs[run_name] = metrics
    
    # ê³µí†µ ë©”íŠ¸ë¦­ ì°¾ê¸°
    common_metrics = set.intersection(*[set(m.keys()) for m in all_runs.values()])
    
    if not common_metrics:
        print("\nâš ï¸  ê³µí†µ ë©”íŠ¸ë¦­ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"\nğŸ“Š ê³µí†µ ë©”íŠ¸ë¦­ ({len(common_metrics)}ê°œ):")
    
    for metric in sorted(common_metrics):
        print(f"\nğŸ“ˆ {metric}")
        print(f"   {'Run':<30} {'ìµœì¢…ê°’':<15} {'ìµœì†Œê°’':<15} {'ìµœëŒ€ê°’':<15}")
        print(f"   {'-'*75}")
        
        for run_name, metrics in all_runs.items():
            if metric in metrics and metrics[metric]:
                vals = [v for _, v in metrics[metric]]
                final_val = vals[-1]
                min_val = min(vals)
                max_val = max(vals)
                
                print(f"   {run_name:<30} {final_val:<15.6f} {min_val:<15.6f} {max_val:<15.6f}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    workspace = Path("/workspace/MiniR1_sogmgm_ver")
    output_dir = workspace / "logs" / "analysis"
    output_dir.mkdir(exist_ok=True, parents=True)
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ TensorBoard ë¡œê·¸ ë””ë ‰í† ë¦¬ ì°¾ê¸°
    log_dirs = [
        workspace / "logs" / "tensorboard_1.7b",
        workspace / "logs" / "tensorboard_4b_lora",
    ]
    
    existing_dirs = [d for d in log_dirs if d.exists()]
    
    if not existing_dirs:
        print("âŒ TensorBoard ë¡œê·¸ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print("ğŸ” ë°œê²¬ëœ TensorBoard ë¡œê·¸:")
    for i, log_dir in enumerate(existing_dirs, 1):
        print(f"   {i}. {log_dir}")
    
    # ê° ë””ë ‰í† ë¦¬ì˜ ë©”íŠ¸ë¦­ ì½ê¸° ë° ë‚´ë³´ë‚´ê¸°
    all_runs_data = {}
    
    for log_dir in existing_dirs:
        print(f"\n{'='*80}")
        print(f"ğŸ“Š {log_dir.name} ë¶„ì„ ì¤‘...")
        print(f"{'='*80}")
        
        metrics = read_tensorboard_events(str(log_dir))
        
        if not metrics:
            print(f"âš ï¸  {log_dir.name}ì—ì„œ ë©”íŠ¸ë¦­ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            continue
        
        print_metric_summary(metrics)
        
        # íŒŒì¼ëª…ì— ì‚¬ìš©í•  run name
        run_name = log_dir.name
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # JSONìœ¼ë¡œ ë‚´ë³´ë‚´ê¸°
        json_path = output_dir / f"{run_name}_{timestamp}.json"
        export_to_json(metrics, str(json_path))
        
        # Markdownìœ¼ë¡œ ë‚´ë³´ë‚´ê¸° (LLM ë¶„ì„ìš©)
        md_path = output_dir / f"{run_name}_{timestamp}.md"
        export_to_markdown(metrics, str(md_path), run_name)
        
        # CSVë¡œ ë‚´ë³´ë‚´ê¸°
        csv_path = output_dir / f"{run_name}_{timestamp}.csv"
        export_to_csv(metrics, str(csv_path))
        
        all_runs_data[run_name] = metrics
    
    # ì—¬ëŸ¬ ì‹¤í–‰ ë¹„êµ (ìˆëŠ” ê²½ìš°)
    if len(all_runs_data) > 1:
        print(f"\n{'='*80}")
        print("ğŸ”„ ì—¬ëŸ¬ ì‹¤í–‰ ë¹„êµ")
        print(f"{'='*80}")
        
        # ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±
        comparison_path = output_dir / f"comparison_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        create_comparison_report(all_runs_data, str(comparison_path))
    
    print("\n" + "="*80)
    print(f"âœ… ë¶„ì„ ì™„ë£Œ! ê²°ê³¼ëŠ” {output_dir}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("="*80)
    print(f"\nğŸ“ ìƒì„±ëœ íŒŒì¼ë“¤:")
    for file in sorted(output_dir.glob("*")):
        print(f"   - {file.name}")


def create_comparison_report(all_runs: Dict[str, Dict[str, List[Tuple[int, float]]]], output_path: str):
    """ì—¬ëŸ¬ ì‹¤í–‰ì„ ë¹„êµí•˜ëŠ” Markdown ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    
    lines = []
    lines.append("# í•™ìŠµ ì‹¤í–‰ ë¹„êµ ë¦¬í¬íŠ¸")
    lines.append(f"\n**ìƒì„± ì‹œê°„:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    lines.append(f"**ë¹„êµ ëŒ€ìƒ:** {', '.join(all_runs.keys())}")
    lines.append("\n---\n")
    
    # ê³µí†µ ë©”íŠ¸ë¦­ ì°¾ê¸°
    common_metrics = set.intersection(*[set(m.keys()) for m in all_runs.values()])
    
    if not common_metrics:
        lines.append("âš ï¸ ê³µí†µ ë©”íŠ¸ë¦­ì´ ì—†ìŠµë‹ˆë‹¤.\n")
    else:
        lines.append(f"## ê³µí†µ ë©”íŠ¸ë¦­ ë¹„êµ ({len(common_metrics)}ê°œ)\n")
        
        for metric in sorted(common_metrics):
            lines.append(f"### ğŸ“Š {metric}\n")
            lines.append("| Run | ìµœì¢…ê°’ | ìµœì†Œê°’ | ìµœëŒ€ê°’ | í‰ê· ê°’ | Step ë²”ìœ„ |")
            lines.append("|-----|--------|--------|--------|--------|-----------|")
            
            for run_name, metrics in all_runs.items():
                if metric in metrics and metrics[metric]:
                    steps, vals = zip(*metrics[metric])
                    final_val = vals[-1]
                    min_val = min(vals)
                    max_val = max(vals)
                    mean_val = np.mean(vals)
                    step_range = f"{min(steps)}-{max(steps)}"
                    
                    lines.append(f"| {run_name} | {final_val:.6f} | {min_val:.6f} | {max_val:.6f} | {mean_val:.6f} | {step_range} |")
            
            lines.append("")
    
    # ê° runë³„ ê³ ìœ  ë©”íŠ¸ë¦­
    lines.append("## Runë³„ ê³ ìœ  ë©”íŠ¸ë¦­\n")
    for run_name, metrics in all_runs.items():
        other_metrics = set()
        for other_run, other_m in all_runs.items():
            if other_run != run_name:
                other_metrics.update(other_m.keys())
        
        unique = set(metrics.keys()) - other_metrics
        if unique:
            lines.append(f"### {run_name}")
            for m in sorted(unique):
                lines.append(f"- {m}")
            lines.append("")
    
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(lines))
    
    print(f"âœ… ë¹„êµ ë¦¬í¬íŠ¸ ì €ì¥: {output_path}")


if __name__ == "__main__":
    main()
