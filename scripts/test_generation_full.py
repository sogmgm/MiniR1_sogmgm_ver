"""
ì™„ì „í•œ ìƒì„± í…ŒìŠ¤íŠ¸ - ëª¨ë“  ì„¤ì • í¬í•¨
ëª¨ë¸ì´ ì œëŒ€ë¡œ ì‘ë™í•˜ëŠ”ì§€ í•™ìŠµ ì „ì— ê²€ì¦
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import random


def test_generation_complete():
    """ì™„ì „í•œ ìƒì„± í…ŒìŠ¤íŠ¸"""
    
    print("="*80)
    print("ğŸ”§ LOADING MODEL...")
    print("="*80)
    
    model_name = "Qwen/Qwen2.5-3B-Instruct"
    
    # 1. í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True,
    )
    
    # 2. pad_token ì„¤ì • (ì¤‘ìš”!)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    print(f"âœ“ Tokenizer loaded")
    print(f"  - Vocab size: {len(tokenizer)}")
    print(f"  - Pad token: {tokenizer.pad_token}")
    print(f"  - EOS token: {tokenizer.eos_token}")
    
    # 3. ëª¨ë¸ ë¡œë“œ
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,  # bf16 ì‚¬ìš©
        device_map="auto"
    )
    
    print(f"âœ“ Model loaded")
    print(f"  - Dtype: {model.dtype}")
    print(f"  - Device: {model.device}")
    print(f"  - Vocab size: {model.config.vocab_size}")
    
    # 4. ê²€ì¦ ë° ë¦¬ì‚¬ì´ì¦ˆ
    if len(tokenizer) != model.config.vocab_size:
        print(f"âš ï¸  Vocab size mismatch!")
        print(f"   Tokenizer: {len(tokenizer)}")
        print(f"   Model: {model.config.vocab_size}")
        print(f"   â†’ Resizing model embeddings...")
        model.resize_token_embeddings(len(tokenizer))
        print(f"   âœ“ Resized to {len(tokenizer)}")
    
    print("\n" + "="*80)
    print("ğŸ“ CREATING PROMPT...")
    print("="*80)
    
    # 5. í…ŒìŠ¤íŠ¸ ë¬¸ì œ (í•™ìŠµ ë°ì´í„°ì™€ ë™ì¼í•œ í˜•ì‹)
    numbers = [75, 25, 3, 1, 7, 10]
    target = 111
    
    messages = [
        {
            "role": "system",
            "content": "You are a helpful assistant. You first think about the reasoning process in <think></think> tags and then provide the answer in <answer></answer> tags."
        },
        {
            "role": "user",
            "content": f"Using the numbers {numbers}, create an equation that equals {target}. "
                      f"You can use basic arithmetic operations (+, -, *, /) and each number can only be used once. "
                      f"Think step by step in <think> tags, then provide your final equation in <answer> tags."
        }
    ]
    
    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    print(prompt)
    print("\n" + "="*80)
    print("ğŸš€ GENERATING (3 attempts with different settings)...")
    print("="*80)
    
    # 6. ì…ë ¥ ì¤€ë¹„
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    print(f"Input shape: {inputs['input_ids'].shape}")
    print(f"Input tokens: {inputs['input_ids'].shape[1]}")
    
    # 7. ì„¸ ê°€ì§€ ì„¤ì •ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
    test_configs = [
        {
            "name": "Safe (repetition_penalty=1.2)",
            "params": {
                "max_new_tokens": 300,
                "temperature": 0.8,
                "top_p": 0.95,
                "top_k": 50,
                "do_sample": True,
                "repetition_penalty": 1.2,  # ğŸ”¥ ë°˜ë³µ ë°©ì§€
                "no_repeat_ngram_size": 3,  # 3-gram ë°˜ë³µ ë°©ì§€
            }
        },
        {
            "name": "Moderate (repetition_penalty=1.1)",
            "params": {
                "max_new_tokens": 300,
                "temperature": 0.7,
                "top_p": 0.9,
                "top_k": 50,
                "do_sample": True,
                "repetition_penalty": 1.1,
                "no_repeat_ngram_size": 2,
            }
        },
        {
            "name": "Original (í•™ìŠµ ì½”ë“œ ì„¤ì •)",
            "params": {
                "max_new_tokens": 300,
                "temperature": 0.7,
                "top_p": 0.9,
                "top_k": 50,
                "do_sample": True,
                # repetition_penalty ì—†ìŒ!
            }
        }
    ]
    
    for i, config in enumerate(test_configs, 1):
        print(f"\n{'='*80}")
        print(f"TEST {i}/3: {config['name']}")
        print(f"{'='*80}")
        
        # ìƒì„±
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                **config['params'],
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
            )
        
        # ë””ì½”ë”©
        completion = tokenizer.decode(
            outputs[0][inputs['input_ids'].shape[1]:], 
            skip_special_tokens=True
        )
        
        # ì¶œë ¥
        print(f"\nğŸ“„ OUTPUT ({len(completion)} chars):")
        print("-"*80)
        print(completion[:500])  # ì²˜ìŒ 500ìë§Œ
        if len(completion) > 500:
            print(f"... (truncated, total {len(completion)} chars)")
        print("-"*80)
        
        # ê²€ì¦
        has_think = "<think>" in completion and "</think>" in completion
        has_answer = "<answer>" in completion and "</answer>" in completion
        has_numbers = any(str(n) in completion for n in numbers)
        has_operators = any(op in completion for op in ['+', '-', '*', '/'])
        is_repetitive = any(c*20 in completion for c in set(completion[:100]))
        
        print(f"\nâœ“ Validation:")
        print(f"  - Has <think> tags: {'âœ…' if has_think else 'âŒ'}")
        print(f"  - Has <answer> tags: {'âœ…' if has_answer else 'âŒ'}")
        print(f"  - Contains problem numbers: {'âœ…' if has_numbers else 'âŒ'}")
        print(f"  - Contains operators: {'âœ…' if has_operators else 'âŒ'}")
        print(f"  - NOT repetitive: {'âœ…' if not is_repetitive else 'âŒ REPETITIVE!'}")
        
        # ì ìˆ˜
        score = sum([has_think, has_answer, has_numbers, has_operators, not is_repetitive])
        print(f"\nğŸ¯ Score: {score}/5")
        
        if score >= 4:
            print("âœ… GOOD - This configuration works!")
        elif score >= 2:
            print("âš ï¸  OKAY - Partially working")
        else:
            print("âŒ BAD - Not working properly")


if __name__ == "__main__":
    test_generation_complete()
