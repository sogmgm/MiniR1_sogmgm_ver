"""
Mini-R1 GRPO Training Script (Weighted Rewards Version)
"""

import argparse
import json
import logging
import os
import time
from datetime import datetime
from pathlib import Path
import yaml
import functools

import torch
from datasets import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    set_seed,
    TrainerCallback,
    TrainerState,
    TrainerControl,
)
from trl import GRPOConfig, GRPOTrainer
from peft import LoraConfig

# reward.pyì—ì„œ í•¨ìˆ˜ import
from rewards import format_reward_func, equation_reward_func, length_penalty_func

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    format="%(asctime)s - %(levelname)s - %(message)s",
    level=logging.INFO,
    datefmt="%Y-%m-%d %H:%M:%S"
)
logger = logging.getLogger(__name__)

# ------------------------------------------------------------------------------
# Helper: ê°€ì¤‘ì¹˜ ì ìš© ë˜í¼ í•¨ìˆ˜
# ------------------------------------------------------------------------------
def get_weighted_reward_func(reward_func, weight, **fixed_kwargs):
    """
    ê¸°ì¡´ ë¦¬ì›Œë“œ í•¨ìˆ˜ì˜ ê²°ê³¼ê°’ì— ê°€ì¤‘ì¹˜(weight)ë¥¼ ê³±í•´ì„œ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜ ìƒì„±
    fixed_kwargs: í•¨ìˆ˜ì— ê³ ì •ìœ¼ë¡œ ì „ë‹¬í•  ì¶”ê°€ íŒŒë¼ë¯¸í„° (ì˜ˆ: max_completion_length)
    """
    @functools.wraps(reward_func)
    def wrapper(*args, **kwargs):
        # fixed_kwargsë¥¼ ë¨¼ì € ì ìš©í•˜ê³ , í˜¸ì¶œ ì‹œ ì „ë‹¬ëœ kwargsë¡œ ë®ì–´ì”€
        merged_kwargs = {**fixed_kwargs, **kwargs}
        rewards = reward_func(*args, **merged_kwargs)
        return [r * weight for r in rewards]
    return wrapper

# ------------------------------------------------------------------------------
# Main Logic
# ------------------------------------------------------------------------------

def load_config(config_path: str) -> dict:
    with open(config_path, 'r') as f:
        return yaml.safe_load(f)

def load_dataset_from_json(file_path: str) -> Dataset:
    with open(file_path, 'r', encoding='utf-8') as f:
        return Dataset.from_list(json.load(f))

def save_training_metrics(metrics: dict, output_file: str):
    output_path = Path(output_file)
    if output_path.exists():
        with open(output_path, 'r') as f:
            all_metrics = json.load(f)
    else:
        all_metrics = []
    all_metrics.append({"timestamp": datetime.now().isoformat(), **metrics})
    output_path.parent.mkdir(parents=True, exist_ok=True)
    with open(output_path, 'w') as f:
        json.dump(all_metrics, f, indent=2)


class MemoryClearCallback(TrainerCallback):
    """
    ì£¼ê¸°ì ìœ¼ë¡œ CUDA ìºì‹œ ì •ë¦¬
    """
    def __init__(self, clear_every_n_steps: int = 10):
        self.clear_every_n_steps = clear_every_n_steps

    def on_step_end(self, args, state: TrainerState, control: TrainerControl, **kwargs):
        if state.global_step % self.clear_every_n_steps == 0:
            torch.cuda.empty_cache()
            logger.debug(f"ğŸ§¹ Cleared CUDA cache at step {state.global_step}")
        return control


class SampleSavingCallback(TrainerCallback):
    """
    í•™ìŠµ ì¤‘ ìƒì„± ìƒ˜í”Œ ì €ì¥ ì½œë°± (ê°€ì¤‘ì¹˜ ì ìš©ëœ ì ìˆ˜ ê³„ì‚° í¬í•¨)
    """
    def __init__(self, save_steps: int, tokenizer, config, eval_dataset, weights, max_completion_length: int):
        self.save_steps = save_steps
        self.tokenizer = tokenizer
        self.config = config
        self.eval_dataset = eval_dataset
        self.weights = weights
        self.max_completion_length = max_completion_length

    def on_step_end(self, args, state: TrainerState, control: TrainerControl, **kwargs):
        step = state.global_step
        if step == 1 or (step % self.save_steps == 0):
            model = kwargs.get('model', None)
            if model and self.tokenizer:
                self._generate_and_save_samples(model, step)
        return control

    def _generate_and_save_samples(self, model, step):
        import random
        try:
            was_training = model.training
            model.eval()
    
            # ìƒ˜í”Œ ë°ì´í„° ì¤€ë¹„
            if self.eval_dataset and len(self.eval_dataset) > 0:
                sample = self.eval_dataset[random.randint(0, len(self.eval_dataset)-1)]
                numbers = sample.get('nums', []) or sample.get('numbers', [])
                target = sample.get('target', None)
            else:
                numbers = random.sample(range(1, 101), 6)
                target = random.randint(10, 999)
    
            # í”„ë¡¬í”„íŠ¸ ìƒì„±
            messages = [
                {"role": "system", "content": "Respond in the following format: <think> ... </think> <answer> ... </answer>"},
                {"role": "user", "content":  f"Create an equation using only the numbers {numbers} that equals {target}. "
                       f"Using the numbers {numbers}, create an equation that equals {target}. You can use basic arithmetic operations (+, -, * or /) and each number should only be used once. Show your work in <think> </think> tags. And return the final equation and answer in <answer> </answer> tags"}
            ]
            prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    
            # ìƒì„±
            inputs = self.tokenizer(prompt, return_tensors="pt").to(model.device)
            with torch.no_grad():
                outputs = model.generate(
                    **inputs, 
                    max_new_tokens=self.max_completion_length,
                    temperature=self.config['grpo'].get('temperature', 0.9),
                    top_p=self.config['grpo'].get('top_p', 0.95),
                    do_sample=True
                )
            
            full_output = self.tokenizer.decode(outputs[0], skip_special_tokens=False)
            completion = full_output[len(prompt):]
    
            # === [ìˆ˜ì •] GRPOTrainerê°€ í˜¸ì¶œí•˜ëŠ” ê²ƒê³¼ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ í˜¸ì¶œ ===
            # prompts, completions, completion_idsë¥¼ ëª¨ë‘ ë¦¬ìŠ¤íŠ¸ë¡œ ì „ë‹¬
            prompts = [prompt]
            completions = [completion]
            completion_ids = [inputs['input_ids'][0].tolist()]  # ì‹¤ì œ í† í° ID
            
            raw_format = format_reward_func(
                prompts,
                completions,
                completion_ids
            )[0]
            
            raw_equation = equation_reward_func(
                prompts,
                completions,
                completion_ids,
                target=[target],
                nums=[numbers]
            )[0]
            
            raw_length = length_penalty_func(
                prompts,
                completions,
                completion_ids,
                max_completion_length=self.max_completion_length
            )[0]
    
            # ê°€ì¤‘ì¹˜ ì ìš©
            w_format = raw_format * self.weights['format']
            w_equation = raw_equation * self.weights['equation']
            w_length = raw_length * self.weights['length']
            
            total_reward = w_format + w_equation + w_length
    
            # íŒŒì¼ ì €ì¥
            samples_dir = Path("logs/generation_samples")
            samples_dir.mkdir(parents=True, exist_ok=True)
            sample_file = samples_dir / f"step_{step:05d}.txt"
    
            with open(sample_file, 'w', encoding='utf-8') as f:
                f.write(f"Step: {step}\nTarget: {target}, Nums: {numbers}\n")
                f.write(f"Generated:\n{completion}\n\n")
                f.write(f"--- Rewards (Weighted) ---\n")
                f.write(f"Format:   {w_format:.2f} (Raw: {raw_format:.2f} * {self.weights['format']})\n")
                f.write(f"Equation: {w_equation:.2f} (Raw: {raw_equation:.2f} * {self.weights['equation']})\n")
                f.write(f"Length:   {w_length:.2f} (Raw: {raw_length:.2f} * {self.weights['length']})\n")
                f.write(f"Total:    {total_reward:.2f}\n")
    
            if was_training: model.train()
            logger.info(f"âœ… Step {step} Sample Saved. Total Reward: {total_reward:.2f}")
    
        except Exception as e:
            logger.error(f"Sample generation failed: {e}")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=str, required=True)
    parser.add_argument("--resume_from_checkpoint", type=str, default=None)
    args = parser.parse_args()
    
    # Config ë¡œë“œ
    config = load_config(args.config)
    set_seed(config.get('dataset', {}).get('shuffle_seed', 42))
    
    # ë°ì´í„°ì…‹ ë¡œë“œ
    dataset_dir = Path(config['dataset']['cache_dir'])
    train_dataset = load_dataset_from_json(str(dataset_dir / "train_countdown_r1.json"))
    test_dataset = load_dataset_from_json(str(dataset_dir / "test_countdown_r1.json"))
    
    # ëª¨ë¸ & í† í¬ë‚˜ì´ì € ë¡œë“œ
    model_name = config['model']['name']
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    if tokenizer.pad_token is None: 
        tokenizer.pad_token = tokenizer.eos_token
    
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        attn_implementation=config['model'].get('attn_implementation', 'eager')
    )
    if len(tokenizer) != model.config.vocab_size:
        model.resize_token_embeddings(len(tokenizer))

    # LoRA ì„¤ì •
    use_peft = config['model'].get('use_peft', False)  # Config ê¸°ë³¸ê°’ì— ë§ì¶¤
    peft_config = None
    if use_peft:
        pc = config.get('peft', {})
        peft_config = LoraConfig(
            r=pc.get('r', 16),
            lora_alpha=pc.get('lora_alpha', 32),
            lora_dropout=pc.get('lora_dropout', 0.05),
            target_modules=pc.get('target_modules', None),
            task_type="CAUSAL_LM",
        )
        logger.info(f"âœ… Using LoRA with r={pc.get('r', 16)}")
    else:
        logger.info("âš ï¸  Full fine-tuning mode (no LoRA)")

    # === [ì¤‘ìš”] ë¦¬ì›Œë“œ ê°€ì¤‘ì¹˜ ë¡œë“œ ë° í•¨ìˆ˜ ë˜í•‘ ===
    reward_weights_config = config.get('reward_weights', {})
    grpo_config = config['grpo']
    max_completion_length = grpo_config['max_completion_length']
    
    # Configì—ì„œ ê°’ ì½ê¸° (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ 1.0)
    w_format = reward_weights_config.get('format_reward', 1.0)
    w_equation = reward_weights_config.get('equation_reward', 1.0)
    w_length = reward_weights_config.get('length_penalty', 1.0)
    
    logger.info(f"{'='*40}")
    logger.info(f"âš–ï¸  Reward Weights Applied:")
    logger.info(f"   - Format:   x {w_format}")
    logger.info(f"   - Equation: x {w_equation}")
    logger.info(f"   - Length:   x {w_length}")
    logger.info(f"{'='*40}")

    # === [ìˆ˜ì •] max_completion_lengthë¥¼ ê³ ì • íŒŒë¼ë¯¸í„°ë¡œ ì „ë‹¬ ===
    weighted_format_func = get_weighted_reward_func(format_reward_func, w_format)
    weighted_equation_func = get_weighted_reward_func(equation_reward_func, w_equation)
    weighted_length_func = get_weighted_reward_func(
        length_penalty_func, 
        w_length, 
        max_completion_length=max_completion_length  # ê³ ì • íŒŒë¼ë¯¸í„°
    )

    # Trainer ì„¤ì •
    training_config = config['training']
    
    training_args = GRPOConfig(
        output_dir=training_config['output_dir'],
        learning_rate=training_config['learning_rate'],
        lr_scheduler_type=training_config['lr_scheduler_type'],
        warmup_ratio=training_config.get('warmup_ratio', 0.05),
        max_steps=training_config['max_steps'],
        logging_steps=training_config['logging_steps'],
        save_steps=training_config['save_steps'],
        eval_steps=training_config.get('eval_steps', 50),  # === [ì¶”ê°€] ===
        per_device_train_batch_size=training_config['per_device_train_batch_size'],
        gradient_accumulation_steps=training_config['gradient_accumulation_steps'],
        gradient_checkpointing=training_config['gradient_checkpointing'],
        bf16=training_config.get('bf16', True),
        max_prompt_length=grpo_config['max_prompt_length'],
        max_completion_length=max_completion_length,
        num_generations=grpo_config['num_generations'],
        temperature=grpo_config.get('temperature', 0.9),  # === [ì¶”ê°€] ===
        top_p=grpo_config.get('top_p', 0.95),  # === [ì¶”ê°€] ===
        beta=grpo_config['beta'],
        logging_dir=training_config.get('logging_dir'),
        report_to=training_config.get('report_to', ['tensorboard']),
        save_total_limit=training_config.get('save_total_limit', 2),
        max_grad_norm=training_config.get('max_grad_norm', 0.5),
        optim=training_config.get('optim', 'adamw_torch'),
        weight_decay=training_config.get('weight_decay', 0.01),
    )
    
    trainer = GRPOTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        peft_config=peft_config,
        reward_funcs=[
            weighted_format_func,
            weighted_equation_func,
            weighted_length_func,  # max_completion_length ì´ë¯¸ ê³ ì •ë¨
        ],
    )
    
    # === [ì¶”ê°€] ì½œë°±ë“¤ ===
    weights_dict = {'format': w_format, 'equation': w_equation, 'length': w_length}
    
    # ìƒ˜í”Œ ì €ì¥ ì½œë°±
    sample_callback = SampleSavingCallback(
        save_steps=config['sampling']['save_samples_every'],
        tokenizer=tokenizer,
        config=config,
        eval_dataset=test_dataset,
        weights=weights_dict,
        max_completion_length=max_completion_length
    )
    trainer.add_callback(sample_callback)
    
    # ë©”ëª¨ë¦¬ ì •ë¦¬ ì½œë°±
    clear_cache_steps = config.get('runpod', {}).get('clear_cache_every_n_steps', 10)
    memory_callback = MemoryClearCallback(clear_every_n_steps=clear_cache_steps)
    trainer.add_callback(memory_callback)
    
    logger.info("ğŸš€ Starting training...")
    trainer.train(resume_from_checkpoint=args.resume_from_checkpoint)
    
    trainer.save_model(training_args.output_dir)
    tokenizer.save_pretrained(training_args.output_dir)
    logger.info("âœ¨ Training completed.")

if __name__ == "__main__":
    main()