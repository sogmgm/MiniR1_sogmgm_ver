#!/usr/bin/env python3
"""
ğŸ” Tokenizer Debug Script
Qwen2.5-3B-Instruct í† í¬ë‚˜ì´ì € ìˆ«ì ì¸ì½”ë”© í…ŒìŠ¤íŠ¸
"""

import torch
from transformers import AutoTokenizer
import json

print("Loading tokenizer...")
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-3B-Instruct")
print(f"âœ… Tokenizer loaded")
print(f"ğŸ“Š Vocab size: {len(tokenizer)}")
print(f"ğŸ”¢ PAD token: {tokenizer.pad_token} (ID: {tokenizer.pad_token_id})")
print(f"ğŸ”¢ EOS token: {tokenizer.eos_token} (ID: {tokenizer.eos_token_id})")

# ============================================================================
# 1ï¸âƒ£ ì‹¤ì œ í”„ë¡¬í”„íŠ¸ í† í¬ë‚˜ì´ì§• í…ŒìŠ¤íŠ¸
# ============================================================================
print("\n" + "="*80)
print("1ï¸âƒ£  ACTUAL PROMPT TOKENIZATION TEST")
print("="*80)

test_prompt = """<|im_start|>system
You are a helpful assistant. You first think about the reasoning process in <think></think> tags and then provide the answer in <answer></answer> tags.<|im_end|>
<|im_start|>user
Using the numbers [75, 25, 3, 1, 7, 10], create an equation that equals 111. You can use basic arithmetic operations (+, -, *, /) and each number can only be used once. Think step by step in <think> tags, then provide your final equation in <answer> tags.<|im_end|>
<|im_start|>assistant"""

print("\nğŸ“ Original Prompt:")
print(test_prompt)

# í† í¬ë‚˜ì´ì§•
tokens = tokenizer.encode(test_prompt)
print(f"\nğŸ”¢ Token IDs ({len(tokens)} tokens):")
print(tokens[:50], "...")

# ë””ì½”ë”©
decoded = tokenizer.decode(tokens)
print("\nğŸ”„ Decoded back:")
print(decoded[:300], "...")

# ì¼ì¹˜ ì—¬ë¶€
if test_prompt == decoded:
    print("\nâœ… PERFECT MATCH!")
else:
    print("\nâŒ MISMATCH DETECTED!")

# ============================================================================
# 2ï¸âƒ£ ìˆ«ì í† í¬ë‚˜ì´ì§• ìƒì„¸ í…ŒìŠ¤íŠ¸
# ============================================================================
print("\n" + "="*80)
print("2ï¸âƒ£  NUMBER TOKENIZATION DETAILED TEST")
print("="*80)

numbers_test = [
    "[75, 25, 3, 1, 7, 10]",
    "75", "25", "3", "1", "7", "10", "111",
    "75 + 25 + 3 + 1 + 7 = 111",
    "(75 + 25) * 3",
    "66", "99", "80",
]

for text in numbers_test:
    tokens = tokenizer.encode(text, add_special_tokens=False)
    decoded = tokenizer.decode(tokens)
    match = "âœ…" if text == decoded else "âŒ MISMATCH!"
    
    print(f"\nInput:   '{text}'")
    print(f"Tokens:  {tokens}")
    print(f"Decoded: '{decoded}'")
    print(f"Status:  {match}")
    
    if len(tokens) > 1 and len(tokens) <= 10:
        print(f"  Token breakdown:")
        for i, tid in enumerate(tokens):
            decoded_token = tokenizer.decode([tid])
            print(f"    [{i}] {tid} -> '{decoded_token}'")

# ============================================================================
# 3ï¸âƒ£ ì „ê° ë¬¸ì ë° íŠ¹ìˆ˜ ìœ ë‹ˆì½”ë“œ í…ŒìŠ¤íŠ¸
# ============================================================================
print("\n" + "="*80)
print("3ï¸âƒ£  FULL-WIDTH AND UNICODE CHARACTER TEST")
print("="*80)

weird_outputs = [
    ("7", "regular"),
    ("ï¼—", "full-width"),
    ("1", "regular"),
    ("ï¼‘", "full-width"),
    ("8", "regular"),
    ("ï¼˜", "full-width"),
]

for text, desc in weird_outputs:
    tokens = tokenizer.encode(text, add_special_tokens=False)
    decoded = tokenizer.decode(tokens)
    
    print(f"\n{desc}: '{text}'")
    print(f"  Unicode: U+{ord(text):04X}")
    print(f"  Bytes:   {text.encode('utf-8').hex()}")
    print(f"  Tokens:  {tokens}")
    print(f"  Decoded: '{decoded}'")
    print(f"  Match:   {'âœ…' if text == decoded else 'âŒ'}")

# ============================================================================
# 4ï¸âƒ£ Vocabulary ë‚´ ìˆ«ì ì¡´ì¬ ì—¬ë¶€ í™•ì¸
# ============================================================================
print("\n" + "="*80)
print("4ï¸âƒ£  VOCABULARY CHECK FOR NUMBERS (0-120)")
print("="*80)

single_token_nums = []
multi_token_nums = []

for num in range(0, 121):
    num_str = str(num)
    token_ids = tokenizer.encode(num_str, add_special_tokens=False)
    
    if len(token_ids) == 1:
        single_token_nums.append((num, token_ids[0]))
    else:
        multi_token_nums.append((num, token_ids))

print(f"\nâœ… Single-token numbers ({len(single_token_nums)}):")
for num, tid in single_token_nums[:20]:
    print(f"  {num:3d} -> Token ID: {tid}")
if len(single_token_nums) > 20:
    print(f"  ... and {len(single_token_nums) - 20} more")

print(f"\nâš ï¸  Multi-token numbers ({len(multi_token_nums)}):")
for num, tids in multi_token_nums[:10]:
    decoded_parts = [tokenizer.decode([tid]) for tid in tids]
    print(f"  {num:3d} -> {tids} = {decoded_parts}")
if len(multi_token_nums) > 10:
    print(f"  ... and {len(multi_token_nums) - 10} more")

# ============================================================================
# 5ï¸âƒ£ íŠ¹ìˆ˜ ë¬¸ì ë° ì—°ì‚°ì í† í¬ë‚˜ì´ì§•
# ============================================================================
print("\n" + "="*80)
print("5ï¸âƒ£  SPECIAL CHARACTERS AND OPERATORS")
print("="*80)

special_chars = ['[', ']', ',', ' ', '+', '-', '*', '/', '=', '(', ')', '<', '>']

for char in special_chars:
    tokens = tokenizer.encode(char, add_special_tokens=False)
    decoded = tokenizer.decode(tokens)
    match = 'âœ…' if char == decoded else 'âŒ'
    print(f"'{char}' -> {tokens} -> '{decoded}' {match}")

# ============================================================================
# 6ï¸âƒ£ ì‹¤ì œ í•™ìŠµ ë°ì´í„° ìƒ˜í”Œ í…ŒìŠ¤íŠ¸
# ============================================================================
print("\n" + "="*80)
print("6ï¸âƒ£  TRAINING DATA SAMPLE TEST")
print("="*80)

try:
    with open('.cache/datasets/train_countdown_r1.json', 'r') as f:
        train_data = json.load(f)
    
    print(f"Total samples: {len(train_data)}")
    
    for i, sample in enumerate(train_data[:2]):
        print(f"\n{'='*80}")
        print(f"Sample {i+1}:")
        print(f"{'='*80}")
        
        prompt = sample['prompt']
        print(f"Prompt (first 200 chars):\n{prompt[:200]}...")
        
        tokens = tokenizer.encode(prompt)
        print(f"\nToken count: {len(tokens)}")
        
        decoded = tokenizer.decode(tokens)
        
        if prompt == decoded:
            print("âœ… Encode/Decode: PERFECT")
        else:
            print("âŒ Encode/Decode: MISMATCH")
            
except FileNotFoundError:
    print("âš ï¸  Training data file not found")

# ============================================================================
# 7ï¸âƒ£ ëª¨ë¸ ìƒì„± ì‹œë®¬ë ˆì´ì…˜
# ============================================================================
print("\n" + "="*80)
print("7ï¸âƒ£  SIMULATING MODEL GENERATION")
print("="*80)

model_outputs = [
    "Given numbers: 85, 40, -15",
    "Target: 99",
    "(7 * (3 *)) + (5 *) + (-1 *)",
]

for output in model_outputs:
    print(f"\nModel output: '{output}'")
    tokens = tokenizer.encode(output, add_special_tokens=False)
    print(f"Tokens: {tokens}")
    decoded = tokenizer.decode(tokens)
    print(f"Decoded: '{decoded}'")
    print(f"Match: {'âœ…' if output == decoded else 'âŒ'}")

print("\n" + "="*80)
print("ğŸ¯ TEST COMPLETE")
print("="*80)
